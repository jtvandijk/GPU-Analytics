{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning, satellite image classification \n",
    "\n",
    "GeoAI, or geospatial artificial intelligence (AI), has become a trending topic and the frontier for\n",
    "spatial analytics in Geography [(Li and Hsu, 2022)](https://www.mdpi.com/2220-9964/11/7/385/pdf). Although the field of AI has experienced highs and lows in the past decades, it has recently gained tremendous momentum because of breakthrough developments in deep (machine) learning, immense available computing power, and the pressing needs for mining and understanding big data. \n",
    "\n",
    "\n",
    "# Objectives \n",
    "\n",
    "The objective of the second *Case Study* is to show case how we can use GPU for satellite image classification. We will be discussing two case studies - (1) training a CNN model from scratch using Pytorch to detect landuse classfication from satellite images (2) using a pretrained computer vision model to underestand the \"scenicness\" of images. While using a GPU is a commonly integreated into deep learning libraries, we will also provide best practices to maximise your training efficiency. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2.1. Classifying EuraSat images using Convolutional Neural Networks (CNNs)\n",
    "\n",
    "\n",
    "\n",
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a type of artificial neural network that are designed to work with grid-structured data, such as an image, a speech signal, or a video. They are particularly effective for image and video classification, object detection and recognition, and natural language processing tasks.\n",
    "\n",
    "The key components of a CNN are convolutional layers, activation functions, pooling layers, and fully connected layers. \n",
    "\n",
    "1. Convolutional layers: Convolutional layers are the building blocks of a CNN. They perform a convolution operation on the input data, where a small matrix (known as a filter or kernel) is moved across the input data, element-wise multiplication is performed between the elements of the filter and the input data, and then the results are summed up to produce a single output value. This process is repeated for every possible position of the filter, resulting in a set of outputs, called feature maps. Convolutional layers can extract features from the input data, such as edges, shapes, textures, etc.\n",
    "\n",
    "2. Activation functions: Activation functions are used to introduce non-linearity into the network. They are applied element-wise to the output of the convolutional layer. The most commonly used activation functions in CNNs are Rectified Linear Unit (ReLU) and sigmoid.\n",
    "\n",
    "3. Pooling layers: Pooling layers are used to reduce the spatial size of the feature maps, making the network less computationally expensive and more robust to changes in the position of objects in the input data. There are several types of pooling, including max pooling and average pooling. In max pooling, the maximum value in a region of the feature map is taken as the output, while in average pooling, the average value in a region is taken as the output.\n",
    "\n",
    "4. Fully connected layers: The fully connected layers are used to make the final prediction using the features extracted by the convolutional and pooling layers. They perform a weighted sum of the inputs, followed by a non-linear activation function, and then produce the final output of the network.\n",
    "\n",
    "\n",
    "The architecture of a CNN can be designed for a specific task by choosing the number of convolutional and fully connected layers, the size of the filters, the type of activation functions, and the type of pooling. The weights of the filters and the biases of the fully connected layers are learned from the training data using an optimization algorithm, such as stochastic gradient descent or Adam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "\n",
    "\n",
    "# python standard modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn standard functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "\n",
    "\n",
    "# standard imports for pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "\n",
    "# torchvision imports\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import seaborn as sns \n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data preparation and pre-processing \n",
    "\n",
    "- Data preperation. Download the EuroSat dataset from Pytorch. EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples. The classes are 'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River' and 'SeaLake'. \n",
    "- Data transformation - To prepare the data for our model, we need to convert image into the data structures that are recognisable on GPUs and Pytorch. We can also do data augumation to increase sample size, but we will not go into this in depth for this tutorial \n",
    "- Defining batch size - We can not pass the whole dataset into our model to train it, because our memory size is fixed and there is a high chance that our training data exceed the memory capacity of CPU or GPU, so we split the dataset into batches and instead of training the model on whole in a single phase. The batch size can be decided according to memory capacity, generally, it takes in power of 2. For example, the batch size can be 16, 32, 64, 128, 256, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#Define data pre-processing steps\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    #Resize images for (64*64)\n",
    "    transforms.Resize((64,64)),\n",
    "    #Converts images into Pytorch tensor \n",
    "    #Pytorch tensors are multi-dimensional arrays that can be processed on GPUs\n",
    "    transforms.ToTensor(), \n",
    "    #Normalise the input data \n",
    "    #input data is transformed by subtracting the mean and dividing by the standard deviation for each channel. \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#Batch size defines the number of samples processed before the model is updated.\n",
    "batch_size = 40 \n",
    "\n",
    "#Loading EuraSAT and transform using the defined function \n",
    "dataset = torchvision.datasets.EuroSAT(root='./data', \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "#Data loader creates a PyTorch data loader for a given dataset. \n",
    "#The data loader provides an efficient way to iterate over the data in the dataset\n",
    "#and apply batch processing during training.      \n",
    "#num_workers: defines the number of threads to use for loading the data. \n",
    "#If shuffle=True, the data loader will randomly shuffle the data before each epoch to ensure that the model sees a different set of samples each time it is trained.\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "#Classes -> we have 10 labels \n",
    "#'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River' 'SeaLake'\n",
    "classes = data_loader.dataset.classes\n",
    "\n",
    "split=len(dataset.targets)/4\n",
    "train_len=int(len(dataset.targets)-split)\n",
    "val_len=int(split) \n",
    "\n",
    "#Spliting dataset in 75% training, 25% for testing  \n",
    "trainset,testset = torch.utils.data.random_split(dataset, [train_len,val_len])\n",
    "\n",
    "#Create dataloader for training and testing dataset \n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2, Exploring images \n",
    "Our dataset consists of images in form of Tensors, imshow() method of matplotlib python library can be used to visualize images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s},' for j in range(batch_size)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create your own Convnet Model for training\n",
    "\n",
    "Let's create a simple CNN model with two convolution layers using Pytorch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Custom class extends the functionality of nn.Module class from PyTorch, \n",
    "#which provides the basic building blocks for creating neural networks in PyTorch. \n",
    "class Net(nn.Module):\n",
    "    #Setting up layers in CNN \n",
    "    def __init__(self):\n",
    "        #Calling function from nn.Module\n",
    "        super().__init__()\n",
    "        #A 2D convolutional layer with 3 input channels, 6 output, and kernel (filter size) size of 5x5 \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        #A max-pooling layer with kernel size 2x2 and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #Another convolution layer with 6 input channels, 16 output channels, and a kernel size of 5x5 \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #Three fully-connected linear layers for processing the output of the second convolution network \n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    #Define the foward pass of the network i.e. the computation performed on each input tensor. \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(Net(), (3,64,64),device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Inspecting CPU/GPU usage with PyTorch Profiler and TensorBoard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function \n",
    "def train(model,data,criterion, optimizer,device ):\n",
    "    # Copy the data to the device the model is on \n",
    "    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n",
    "\n",
    "    #Predict the output for given input\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    #Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    #Clear the previous gradients, compute gradients of all variables wrt loss\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Backpropagation, update weights\n",
    "    loss.backward()\n",
    "\n",
    "    #Update the parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU ----------------------------\n",
    "#Initialise model \n",
    "#Define device on cuda:0 \n",
    "device = torch.device('cuda:0') \n",
    "model = Net().to(device=device)\n",
    "#Define loss function \n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss \n",
    "\n",
    "#Define optimizer function \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function's value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 \n",
    "\n",
    "#Set random seed for reproducibility\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#Profiler\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4), \n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/gpu_profile'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU ----------------------------\n",
    "#Reinitialise model, loss function, optimizer and random seed \n",
    "device = torch.device('cpu')\n",
    "model = Net().to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4),\n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/cpu_profile'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing model performance with a fine-tune model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_bn = torchvision.models.vgg11_bn(weights=True)\n",
    "# Freeze weights of all layers except the new classification layer\n",
    "for param in vgg11_bn.parameters():\n",
    "    param.requires_grad = False \n",
    "num_ftrs = vgg11_bn.classifier[6].in_features\n",
    "# Replace the final classfication layer \n",
    "vgg11_bn.classifier[6] = nn.Linear(num_ftrs,len(classes))\n",
    "vgg11_bn.classifier[6].requires_grad = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16:GPU --------\n",
    "\n",
    "#GPU ----------------------------\n",
    "#Initialise model \n",
    "#Define device on cuda:0 \n",
    "device = torch.device('cuda:0') \n",
    "#Change model to vgg11_bn\n",
    "model = vgg11_bn.to(device=device)\n",
    "#Define loss function \n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss \n",
    "\n",
    "#Define optimizer function \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function's value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 \n",
    "\n",
    "#Set random seed for reproducibility\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#Profiler\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4), \n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/gpu_vgg'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16:CPU --------\n",
    "\n",
    "#CPU ----------------------------\n",
    "#Initialise model \n",
    "#Define device on cuda:0 \n",
    "device = torch.device('cpu')\n",
    "#Change model to vgg11_bn\n",
    "model = vgg11_bn.to(device=device)\n",
    "#Define loss function \n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss \n",
    "\n",
    "#Define optimizer function \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function's value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 \n",
    "\n",
    "#Set random seed for reproducibility\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#Profiler\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4), \n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/cpu_vgg'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, device=device):\n",
    "    \n",
    "    # Initialize time \n",
    "    since = time.time()\n",
    "\n",
    "    # Initialize reporting metrics \n",
    "    train_acc_history,val_acc_history,train_loss_history,val_loss_history = [],[],[],[]\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "        \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data).type(torch.float).item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)     \n",
    "                train_loss_history.append(epoch_loss)\n",
    "            else: \n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    " \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "    return train_acc_history,val_acc_history,train_loss_history,val_loss_history,time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Dictionary of dataloader \n",
    "dataloader_all = {}\n",
    "dataloader_all['train'] = train_loader\n",
    "dataloader_all['val'] = test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16\n",
    "#CPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cpu') \n",
    "vgg11_cpu = vgg11_bn.to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg11_cpu.parameters(), lr=0.01, momentum=0.9) \n",
    "train_acc_history,val_acc_history,train_loss_history,val_loss_history,vgg_cpu_time  = train_model(vgg11_cpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device) \n",
    "#GPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cuda:0') \n",
    "vgg11_gpu = vgg11_bn.to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(vgg11_gpu.parameters(), lr=0.01, momentum=0.9) \n",
    "train_acc_history,val_acc_history,train_loss_history,val_loss_history,vgg_gpu_time  = train_model(vgg11_gpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "#CPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cpu')\n",
    "base_cpu = Net().to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(base_cpu.parameters(), lr=0.01, momentum=0.9)\n",
    "base_train_acc_history,base_val_acc_history,base_train_loss_history,base_val_loss_history,baseline_cpu_time  = train_model(base_cpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device)\n",
    "#GPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cuda:0')\n",
    "base_gpu = Net().to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(base_gpu.parameters(), lr=0.01, momentum=0.9)\n",
    "base_train_acc_history,base_val_acc_history,base_train_loss_history,base_val_loss_history,baseline_gpu_time  = train_model(base_gpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot compute time \n",
    "compute_time = pd.DataFrame([baseline_cpu_time,baseline_gpu_time,vgg_cpu_time,vgg_gpu_time],columns=['Time'])\n",
    "compute_time['Model'] = ['baseline','baseline','VGG16','VGG16']\n",
    "compute_time['Mode'] = ['CPU','GPU','CPU','GPU']\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "g= sns.catplot(data=compute_time, kind='bar',x='Model',y='Time',hue='Mode')\n",
    "g.set_axis_labels(\"\", \"Total Time (Seconds)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1,figsize=(20,20))\n",
    "def show_heatmap(test_loader,model,ax,name):\n",
    "    heatmap = pd.DataFrame(data=0,index=classes,columns=classes)\n",
    "    with torch.no_grad():\n",
    "        number_corrects = 0\n",
    "        number_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            number_corrects += (predicted==labels).sum().item()\n",
    "            number_samples += labels.size(0)\n",
    "            for i in range(len(labels)):\n",
    "                true_label = labels[i].item()\n",
    "                predicted_label = predicted[i].item()\n",
    "                heatmap.iloc[true_label,predicted_label] += 1\n",
    "    sns.heatmap(heatmap, annot=True, fmt=\"d\",cmap=\"YlGnBu\",ax=ax) \n",
    "    ax.set_title(f'{name}, Overall accuracy {(number_corrects / number_samples)*100}%') \n",
    "\n",
    "show_heatmap(test_loader,vgg11_gpu,ax[0],\"VGG16\")\n",
    "show_heatmap(test_loader,base_gpu,ax[1],\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Loss  \n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "vgg_loss_array = zip(train_loss_history,val_loss_history)\n",
    "vgg_loss_df=pd.DataFrame(vgg_loss_array,columns=['train','test'])\n",
    "vgg_loss_df.plot(ax=ax[0])\n",
    "ax[0].set_title('vgg16')\n",
    "ax[0].set_ylim(0,2)\n",
    "\n",
    "baseline_loss_array = zip(base_train_loss_history,base_val_loss_history)\n",
    "baseline_loss_df = pd.DataFrame(baseline_loss_array,columns=['train','test'])\n",
    "baseline_loss_df.plot(ax=ax[1])\n",
    "ax[1].set_title('Baseline')\n",
    "ax[1].set_ylim(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Accuracy \n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "vgg_acc_array = zip(train_acc_history,val_acc_history)\n",
    "vgg_acc_df=pd.DataFrame(vgg_acc_array,columns=['train','test'])\n",
    "vgg_acc_df.plot(ax=ax[0])\n",
    "ax[0].set_title('vgg16')\n",
    "ax[0].set_ylim(0,1)\n",
    "\n",
    "baseline_acc_array = zip(base_train_acc_history,base_val_acc_history)\n",
    "baseline_acc_df = pd.DataFrame(baseline_acc_array,columns=['train','test'])\n",
    "baseline_acc_df.plot(ax=ax[1])\n",
    "ax[1].set_title('Baseline')\n",
    "ax[1].set_ylim(0,1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f42c824b77f1e2e1513e49dcb1c632653d6eadd5ba6d14e93383243b865c1d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
