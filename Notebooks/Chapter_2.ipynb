{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning, satellite image classification \n",
    "\n",
    "GeoAI, or geospatial artificial intelligence (AI), has become a trending topic and the frontier for\n",
    "spatial analytics in Geography [(Li and Hsu, 2022)](https://www.mdpi.com/2220-9964/11/7/385/pdf). Although the field of AI has experienced highs and lows in the past decades, it has recently gained tremendous momentum because of breakthrough developments in deep (machine) learning, immense available computing power, and the pressing needs for mining and understanding big data. \n",
    "\n",
    "\n",
    "# Objectives \n",
    "\n",
    "The objective of the second Case Study is to showcase how we can use GPU for satellite image classification. We will be discussing two case studies - (1) training a CNN model from scratch using Pytorch to detect land use classification from satellite images (2) comparing model performance with a fine-tuned VGG16 model.\n",
    "\n",
    "While using a GPU is a commonly integrated into deep learning libraries, we will also provide best practices for maximizing your training efficiency.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 1. Classifying EuraSat images using Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python standard modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn standard functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "\n",
    "\n",
    "# standard imports for pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "\n",
    "# torchvision imports\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and pre-processing \n",
    "\n",
    "- Data preperation. Download the EuroSat dataset from Pytorch. EuroSAT dataset is based on Sentinel-2 satellite images covering 13 spectral bands and consisting of 10 classes with 27000 labeled and geo-referenced samples. The classes are 'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River' and 'SeaLake'. \n",
    "- Data transformation - To prepare the data for our model, we need to convert image into the data structures that are recognisable on GPUs and Pytorch. We can also do data augumation to increase sample size, but we will not go into this in depth for this tutorial \n",
    "- Defining batch size - We can not pass the whole dataset into our model to train it, because our memory size is fixed and there is a high chance that our training data exceed the memory capacity of CPU or GPU, so we split the dataset into batches and instead of training the model on whole in a single phase. The batch size can be decided according to memory capacity, generally, it takes in power of 2. For example, the batch size can be 16, 32, 64, 128, 256, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#Define data pre-processing steps\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    #Resize images for (64*64)\n",
    "    transforms.Resize((64,64)),\n",
    "    #Converts images into Pytorch tensor \n",
    "    #Pytorch tensors are multi-dimensional arrays that can be processed on GPUs\n",
    "    transforms.ToTensor(), \n",
    "    #Normalise the input data \n",
    "    #input data is transformed by subtracting the mean and dividing by the standard deviation for each channel. \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#Batch size defines the number of samples processed before the model is updated.\n",
    "batch_size = 40 \n",
    "\n",
    "#Loading EuraSAT and transform using the defined function \n",
    "dataset = torchvision.datasets.EuroSAT(root='./data', \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "#Data loader creates a PyTorch data loader for a given dataset. \n",
    "#The data loader provides an efficient way to iterate over the data in the dataset\n",
    "#and apply batch processing during training.      \n",
    "#num_workers: defines the number of threads to use for loading the data. \n",
    "#If shuffle=True, the data loader will randomly shuffle the data before each epoch to ensure that the model sees a different set of samples each time it is trained.\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "#Classes -> we have 10 labels \n",
    "#'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River' 'SeaLake'\n",
    "classes = data_loader.dataset.classes\n",
    "\n",
    "split=len(dataset.targets)/4\n",
    "train_len=int(len(dataset.targets)-split)\n",
    "val_len=int(split) \n",
    "\n",
    "#Spliting dataset in 75% training, 25% for testing  \n",
    "trainset,testset = torch.utils.data.random_split(dataset, [train_len,val_len])\n",
    "\n",
    "#Create dataloader for training and testing dataset \n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring images \n",
    "Our dataset consists of images in form of Tensors, imshow() method of matplotlib python library can be used to visualize images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "from PIL import Image\n",
    "\n",
    "ROOT_dir = './data/eurosat/2750'\n",
    "folders = os.listdir(ROOT_dir)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "for i, label in enumerate(folders):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    file_path = os.listdir(\"{}/{}\".format(ROOT_dir,label))\n",
    "    image_ = Image.open(ROOT_dir+\"/\"+label+\"/\"+file_path[random.randint(1,100)])\n",
    "    plt.imshow(image_)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your CNN model for training #\n",
    "\n",
    "Let's create a simple CNN model with two convolution layers using Pytorch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Custom class extends the functionality of nn.Module class from PyTorch, \n",
    "#which provides the basic building blocks for creating neural networks in PyTorch. \n",
    "class Net(nn.Module):\n",
    "    #Setting up layers in CNN \n",
    "    def __init__(self):\n",
    "        #Calling function from nn.Module\n",
    "        super().__init__()\n",
    "        #A 2D convolutional layer with 3 input channels, 6 output, and kernel (filter size) size of 5x5 \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        #A max-pooling layer with kernel size 2x2 and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #Another convolution layer with 6 input channels, 16 output channels, and a kernel size of 5x5 \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #Three fully-connected linear layers for processing the output of the second convolution network \n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    #Define the foward pass of the network i.e. the computation performed on each input tensor. \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(Net(), (3,64,64),device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting CPU/GPU usage with PyTorch Profiler and TensorBoard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function \n",
    "def train(model,data,criterion, optimizer,device ):\n",
    "    # Copy the data to the device the model is on \n",
    "    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n",
    "\n",
    "    #Predict the output for given input\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    #Compute the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    #Clear the previous gradients, compute gradients of all variables wrt loss\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #Backpropagation, update weights\n",
    "    loss.backward()\n",
    "\n",
    "    #Update the parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU ----------------------------\n",
    "#Initialise model \n",
    "#Define device on cuda:0 \n",
    "device = torch.device('cuda:0') \n",
    "model = Net().to(device=device)\n",
    "#Define loss function \n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss \n",
    "\n",
    "#Define optimizer function \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function's value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 \n",
    "\n",
    "#Set random seed for reproducibility\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#Profiler\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4), \n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/gpu_profile'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU ----------------------------\n",
    "#Reinitialise model, loss function, optimizer and random seed \n",
    "device = torch.device('cpu')\n",
    "model = Net().to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4),\n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/cpu_profile'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 2. Comparing model performance with a fine-tune model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_bn = torchvision.models.vgg11_bn(weights=True)\n",
    "# Freeze weights of all layers except the new classification layer\n",
    "for param in vgg11_bn.parameters():\n",
    "    param.requires_grad = False \n",
    "num_ftrs = vgg11_bn.classifier[6].in_features\n",
    "# Replace the final classfication layer \n",
    "vgg11_bn.classifier[6] = nn.Linear(num_ftrs,len(classes))\n",
    "vgg11_bn.classifier[6].requires_grad = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16:GPU --------\n",
    "\n",
    "#GPU ----------------------------\n",
    "#Initialise model \n",
    "#Define device on cuda:0 \n",
    "device = torch.device('cuda:0') \n",
    "#Change model to vgg11_bn\n",
    "model = vgg11_bn.to(device=device)\n",
    "#Define loss function \n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss \n",
    "\n",
    "#Define optimizer function \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function's value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 \n",
    "\n",
    "#Set random seed for reproducibility\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#Profiler\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4), \n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/gpu_vgg'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16:CPU --------\n",
    "\n",
    "#CPU ----------------------------\n",
    "#Initialise model \n",
    "#Define device on cuda:0 \n",
    "device = torch.device('cpu')\n",
    "#Change model to vgg11_bn\n",
    "model = vgg11_bn.to(device=device)\n",
    "#Define loss function \n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()#Loss function computes the value between the predicted values and the labels. In this case, we are using Cross-Entropy loss, but many other loss functions are also avaible from nn. Such as focal loss \n",
    "\n",
    "#Define optimizer function \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #Optimizer function aims to reduce the loss function's value by changing the weight vector values through backpropagation in neural networks. We are using Stochastic gradient decent as our optimiser, with learning rate 0.01 and momentum 0.9 \n",
    "\n",
    "#Set random seed for reproducibility\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "#Profiler\n",
    "with torch.profiler.profile(\n",
    "       schedule=torch.profiler.schedule(\n",
    "        wait=2,\n",
    "        warmup=2,\n",
    "        active=3,\n",
    "        repeat=4), \n",
    "        #Saving the profiling logs to a file that can be used by TensorBoard \n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/cpu_vgg'),\n",
    "        profile_memory=True,\n",
    "        ) as prof:\n",
    "    for step, batch_data in enumerate(train_loader,0):\n",
    "        if step >= (2 + 2 + 3) * 4:\n",
    "            break\n",
    "        train(model =model , data =batch_data, criterion = loss_fn, optimizer = optimizer,device=device)\n",
    "        prof.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, device=device):\n",
    "    \n",
    "    # Initialize time \n",
    "    since = time.time()\n",
    "\n",
    "    # Initialize reporting metrics \n",
    "    train_acc_history,val_acc_history,train_loss_history,val_loss_history = [],[],[],[]\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "        \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data).type(torch.float).item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)     \n",
    "                train_loss_history.append(epoch_loss)\n",
    "            else: \n",
    "                val_acc_history.append(epoch_acc)\n",
    "                val_loss_history.append(epoch_loss)\n",
    " \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "\n",
    "    return train_acc_history,val_acc_history,train_loss_history,val_loss_history,time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of dataloader \n",
    "dataloader_all = {}\n",
    "dataloader_all['train'] = train_loader\n",
    "dataloader_all['val'] = test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16\n",
    "#CPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cpu') \n",
    "vgg11_cpu = vgg11_bn.to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vgg11_cpu.parameters(), lr=0.01, momentum=0.9) \n",
    "train_acc_history,val_acc_history,train_loss_history,val_loss_history,vgg_cpu_time  = train_model(vgg11_cpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device) \n",
    "#GPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cuda:0') \n",
    "vgg11_gpu = vgg11_bn.to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(vgg11_gpu.parameters(), lr=0.01, momentum=0.9) \n",
    "train_acc_history,val_acc_history,train_loss_history,val_loss_history,vgg_gpu_time  = train_model(vgg11_gpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "#CPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cpu')\n",
    "base_cpu = Net().to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(base_cpu.parameters(), lr=0.01, momentum=0.9)\n",
    "base_train_acc_history,base_val_acc_history,base_train_loss_history,base_val_loss_history,baseline_cpu_time  = train_model(base_cpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device)\n",
    "#GPU----------------------------------------------------------------------------------------------------------------\n",
    "device = torch.device('cuda:0')\n",
    "base_gpu = Net().to(device=device)\n",
    "loss_fn =  nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.SGD(base_gpu.parameters(), lr=0.01, momentum=0.9)\n",
    "base_train_acc_history,base_val_acc_history,base_train_loss_history,base_val_loss_history,baseline_gpu_time  = train_model(base_gpu,dataloader_all,loss_fn, optimizer, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot compute time \n",
    "compute_time = pd.DataFrame([baseline_cpu_time,baseline_gpu_time,vgg_cpu_time,vgg_gpu_time],columns=['Time'])\n",
    "compute_time['Model'] = ['baseline','baseline','VGG16','VGG16']\n",
    "compute_time['Mode'] = ['CPU','GPU','CPU','GPU']\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "g= sns.catplot(data=compute_time, kind='bar',x='Model',y='Time',hue='Mode')\n",
    "g.set_axis_labels(\"\", \"Total Time (Seconds)\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1,figsize=(20,20))\n",
    "def show_heatmap(test_loader,model,ax,name):\n",
    "    heatmap = pd.DataFrame(data=0,index=classes,columns=classes)\n",
    "    with torch.no_grad():\n",
    "        number_corrects = 0\n",
    "        number_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            number_corrects += (predicted==labels).sum().item()\n",
    "            number_samples += labels.size(0)\n",
    "            for i in range(len(labels)):\n",
    "                true_label = labels[i].item()\n",
    "                predicted_label = predicted[i].item()\n",
    "                heatmap.iloc[true_label,predicted_label] += 1\n",
    "    sns.heatmap(heatmap, annot=True, fmt=\"d\",cmap=\"YlGnBu\",ax=ax) \n",
    "    ax.set_title(f'{name}, Overall accuracy {(number_corrects / number_samples)*100}%') \n",
    "\n",
    "show_heatmap(test_loader,vgg11_gpu,ax[0],\"VGG16\")\n",
    "show_heatmap(test_loader,base_gpu,ax[1],\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Loss  \n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "vgg_loss_array = zip(train_loss_history,val_loss_history)\n",
    "vgg_loss_df=pd.DataFrame(vgg_loss_array,columns=['train','test'])\n",
    "vgg_loss_df.plot(ax=ax[0])\n",
    "ax[0].set_title('vgg16')\n",
    "ax[0].set_ylim(0,2)\n",
    "\n",
    "baseline_loss_array = zip(base_train_loss_history,base_val_loss_history)\n",
    "baseline_loss_df = pd.DataFrame(baseline_loss_array,columns=['train','test'])\n",
    "baseline_loss_df.plot(ax=ax[1])\n",
    "ax[1].set_title('Baseline')\n",
    "ax[1].set_ylim(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Accuracy \n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "vgg_acc_array = zip(train_acc_history,val_acc_history)\n",
    "vgg_acc_df=pd.DataFrame(vgg_acc_array,columns=['train','test'])\n",
    "vgg_acc_df.plot(ax=ax[0])\n",
    "ax[0].set_title('vgg16')\n",
    "ax[0].set_ylim(0,1)\n",
    "\n",
    "baseline_acc_array = zip(base_train_acc_history,base_val_acc_history)\n",
    "baseline_acc_df = pd.DataFrame(baseline_acc_array,columns=['train','test'])\n",
    "baseline_acc_df.plot(ax=ax[1])\n",
    "ax[1].set_title('Baseline')\n",
    "ax[1].set_ylim(0,1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f42c824b77f1e2e1513e49dcb1c632653d6eadd5ba6d14e93383243b865c1d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
