---
title: "Chapter 1 - Address matching and Geocoding address"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

## Addresses and Geocoding textual datasets using GPU 

Address matching is an example of a broad class of data science problems known as data linkage. Data linkage problems involve connecting two datasets together by establishing an association between two records based on one or more common properties. In the case of address matching, the common property is the address itself. However, due to the lack of a universal standard for the correct form of the address, as well as the fact that sources commonly store address data in many forms, matching addresses becomes a significantly complicated task that requires a great deal of data preparation and processing. Table 1 provides a summary of the linkage problem:



|Input address string| Reference data to match against| 
| --- | ----------- |
|Unstructured text |Structured, tokenised|
|Messy, containing typos and abbreviations|Clean, standardised, correct (In most cases)| 
|Incomplete| Snapshot of addresses at a given time| 
|Range from historic to very recent addresses, including businesses| Organisation / business names are not always part of the address| 

Table 1: Summary of the address matching problem, [Office for National Statistics](https://www.ons.gov.uk/methodology/methodologicalpublications/generalmethodology/onsworkingpaperseries/onsworkingpaperseriesno17usingdatasciencefortheaddressmatchingservice#:~:text=Address%20matching%20is%20an%20example,common%20property%20of%20the%20data.) 


As we can see, the address matching problem is a very complex problem, the complexity of which is compounded by the fact that data is growing at an exponential rate. Therefore, the objective of this chapter is to demonstrate how to fast we can use GPU to solve the address matching at each step. 

## Address matching pipeline 

The address matching process can be split into three high-level steps:

### Data pre-processing 
At the most fundamental level, we need to prepare the data for the matching process. There are potentially different approach to do this, but the most common approach is to concatenate (join) the address into its constituent parts. Alternatively, the input address can be split into corresponding parts, such as the street name, house number, postcode, and so on. The former approach is more common, but it ignores the information in the data about the address, and makes it impossible to rely on the natural structure of the address to help match the desired address with the input string. The latter approach is more complex, but flexible, it allows for more accurate comparison because comparing tokens precludes the possibility of the same word representing one element of the address being compared against an unrelated element of the address. 


### Candidate address retrieval
In the second step, we need a method to compare the input address with the reference data. The most common approach is to compare each token in the input address with each token in the reference data. This approach is simple, but it can't address the problem of typos and abbreviations. Alternatively, we can use a probabilistic data matching algorithm, such as Fuzzy string matching to accommodate for typos. For instance, “Birmingham ~ Birmingam” is one letter different (i.e. Levenshtein distance). However, in practice, when each record is compared against every other record, the number of comparisons can be very large, thus leading to a very expensive computation. Or instead, we can consider using a Natural Language Processing (NLP) approach to compare the addresses by assuming that the address is a sequence of unstructured text. One of the most common approaches is to convert the address into a vector and then compare the vector similarity (e.g., [TF-IDF](http://www.tfidf.com/)). 

### Scoring and ranking 

The last step in the process is to evaluate and convey quality of the match between the input and the retrieved candidate address in such a way that would be easily understood and useful for users. We can evaluate the quality of the match by comparing the input address with the candidate address. The most common approach is to use a similarity score to evaluate the similarity among all potential candidate addresses. Depending on the application, the similarity score can be a simple percentage or a more complex score. For instance, we can define a threshold for the similarity score, and only return the candidate addresses that have a similarity score above the threshold. We can also evaluate the model performance by validating the results with a ground truth dataset (e.g., address's unique property reference number (UPRN)). 



## Considerations for GPU 

The address matching problem is a computationally intensive problem. The complexity of the problem is compounded by the fact that data is growing at an exponential rate. A core challenge is to understand which part of the process is the most computationally intensive and which part of the process can be parallelized on the GPU.

Figure 1 illustrates a potential data pipeline for the address matching problem. In the first part, we can concatenate the address from its constituent parts, whereby treating each address as a sequence of unstructured text. In the second part, we can use a character-based n-gram TF-IDF to convert the address into a vector. While the terms in TF-IDF are usually words, this is not a requirement. We can use n-grams: sequences of N continuous characters, to convert the address into a vector representation based on the character level. In the third part, we can use a similarity score to evaluate the similarity among all potential candidate addresses. In this case, we can use k-Nearest Neighbors (kNN) to find the k nearest neighbors of the input address, and then return the candidate addresses that have a consine similarity score above the threshold. 

Using TF-IDF with N-Grams as terms to find similar strings transforms the problem into a matrix multiplication problem, which is computationally much cheaper. This approach can significantly reduce the **memory** it takes to compare strings, as compared to the fuzzy string matching algorithm with TF-IDF and nearest neighbors algorithm. Furthermore, using a GPU for matrix multiplication can further enhance the **speed of comparison**. The greater the Levenshtein distance, the more different the strings are. By using a GPU, we can reduce the time it takes to calculate the Levenshtein distance, making the comparison of strings much faster.


![Figure 1: Address matching pipeline](/images/Case_Study/Chapter_1/Address_matching_pipeline.png)

An example of character-based n-gram TF-IDF
```python
text = "Birmingham" 

# Create a list of n-grams
n_grams = [text[i:i+3] for i in range(len(text)-2+1)]

print(n_grams)
#['Bir', 'irm', 'rmi', 'min', 'ing', 'ngh', 'gha', 'ham']

```

## Data 
In this tutorial, we will be using US hospital data. In these examples, we have two data sets. The [first](https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_account_info.csv) is an internal data set that contains basic hospital account number, name and ownership information. The [second](https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_reimbursement.csv) data set contains hospital information (called provider) as well as the number of discharges and Medicare payment for a specific Heart Failure procedure. The goal is to match up the hospital reimbursement information with the first data so we have more information to analyze our hospital customers. In this instance, we have 5339 hospital accounts and 2697 hospitals with reimbursement information. 


## Step 1: Import Libraries and Data pre-processing 

For the first step, let's compare the speed of computing TF-IDF using Sklearn and RAPIDS cuML module. To make the difference more distinguishable, let's test how the increase of vocabulary size would affect the performance in multiple runs.

```python
# Import libraries 
import cudf
import pandas as pd
from cuml.feature_extraction.text import TfidfVectorizer as GPU_TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer as CPU_TfidfVectorizer
import time 
import numpy as np
import seaborn as sns 
import matplotlib.pyplot as plt

# Import data
data1_url = "https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_account_info.csv"
data2_url = "https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_reimbursement.csv"

account = pd.read_csv(data1_url) #Hospital account information
reimbursement = pd.read_csv(data2_url) #Hospital reimbursement information

#Converting facility name, address, city and state into one string 
account_full_address = account.apply(lambda x: " ".join([x['Facility Name'],x['Address'],x['City'],x['State']]), axis=1).tolist() 

reimbursement_full_address = reimbursement.apply(lambda x: " ".join([x['Provider Name'],x['Provider Street Address'],x['Provider City'],x['Provider State']]), axis=1).tolist() 

```

## Step 2: TF-IDF Vectorization - experimenting the effect of data size on computational time

In this step, we will be experimenting the effect of data size on computational time. We will be using the same data set, but we will be increasing the size of the data set by x times in each run. We will be using the same vocabulary size for each run. 

```python
#Inititate sklearn vectoriser and cuml vectosier 

#CPU vectorizer from sklearn 
cpu_vectorizer = CPU_TfidfVectorizer(analyzer='char',ngram_range=(1,2))
#GPU vectorizer from cuml 
gpu_vectorizer = GPU_TfidfVectorizer(analyzer='char',ngram_range=(1,2))

#Here analyzer='char' means we are using character as the input 
#ngram_range = (1,2) means we are looking at both unigram and bigram for the model input 


#Manually inflating number of rows with 10 run times 
total_datasize = []
cpu_time = []
gpu_time = [] 
for run in range(1,10):
  for i in range(1,50):
    #Manually inflating the number of records 
    input = reimbursement_full_address*i
    total_datasize.append(len(input))
    #Cpu runtime 
    start = time.time()
    cpu_output = cpu_vectorizer.fit_transform(input)
    done = time.time()
    elapsed = done - start 
    cpu_time.append(elapsed)

    #gpu runtime 
    start = time.time()
    #Convert input to cudf series 
    gpu_output = gpu_vectorizer.fit_transform(cudf.Series(input))
    done = time.time()
    elapsed = done - start 
    gpu_time.append(elapsed)

#Create a dataframe to store the results
gpu_elapsed = pd.DataFrame({"time":gpu_time,"data_size":total_datasize,'label':"gpu"})
cpu_elapsed = pd.DataFrame({"time":cpu_time,"data_size":total_datasize,'label':"cpu"})
result = pd.concat([gpu_elapsed,cpu_elapsed]).reset_index()
``` 


```python
fig, ax = plt.subplots(figsize=(10,10))
sns.lineplot(x= 'data_size',y='time',hue = 'label',data = result,ax = ax )
plt.xlabel('Data Size')
plt.ylabel("Time Elapsed ")
plt.title("Comparing the speed of TF-IDF vectorisation on CPU and GPU")
plt.show()
print(cpu_output.shape) #(132153, 874)
```


<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/tf_idf_vec.png?raw=true">
    <figcaption>
    <b>Figure 2: Comparing the speed of TF-IDF vectorisation on CPU and GPU. 
    </b> 
    </figcaption>
    </center>
</figure>


On average, it takes less than 0.25 seconds to process around 130 thousand addresses on the GPU. In comparison, it takes 4.5 seconds on CPUs for the same input dataset. Furthermore, we can also observe that the run time of scikit-learn’s TfidfVectorizer exponetiates as the input data size increases, whereas the performace with CuML is much more stable.  


Now that we have our TF-IDF matrix, we want to query it with some keywords to find the most relevant documents. To do this, we will rely on the fact that our TfidfVectorizer has the capability to vectorize any document according to the vocabulary from the input corpus. The document with the smallest distance (or highest similarity) to our query is the most relevant document to our keywords. We chose to use the cosine similarity. When both vectors are normalized, the cosine similarity is just the dot product of those vectors which correspond to the cosine of the angle between the two vectors.

