---
title: "Chapter 1 - Address matching and Geocoding address"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

## Addresses and Geocoding textual datasets using GPU 

Address matching is an example of a broad class of data science problems known as data linkage. Data linkage problems involve connecting two datasets together by establishing an association between two records based on one or more common properties. In the case of address matching, the common property is the address itself. However, due to the lack of a universal standard for the correct form of the address, as well as the fact that sources commonly store address data in many forms, matching addresses becomes a significantly complicated task that requires a great deal of data preparation and processing. Table 1 provides a summary of the linkage problem:



|Input address string| Reference data to match against| 
| --- | ----------- |
|Unstructured text |Structured, tokenised|
|Messy, containing typos and abbreviations|Clean, standardised, correct (In most cases)| 
|Incomplete| Snapshot of addresses at a given time| 
|Range from historic to very recent addresses, including businesses| Organisation / business names are not always part of the address| 

Table 1: Summary of the address matching problem, [Office for National Statistics](https://www.ons.gov.uk/methodology/methodologicalpublications/generalmethodology/onsworkingpaperseries/onsworkingpaperseriesno17usingdatasciencefortheaddressmatchingservice#:~:text=Address%20matching%20is%20an%20example,common%20property%20of%20the%20data.) 


As we can see, the address matching problem is a very complex problem, the complexity of which is compounded by the fact that data is growing at an exponential rate. Therefore, the objective of this chapter is to demonstrate how to fast we can use GPU to solve the address matching at each step. 

## Address matching pipeline 

The address matching process can be split into three high-level steps:

### Data pre-processing 
At the most fundamental level, we need to prepare the data for the matching process. There are potentially different approach to do this, but the most common approach is to concatenate (join) the address into its constituent parts. Alternatively, the input address can be split into corresponding parts, such as the street name, house number, postcode, and so on. The former approach is more common, but it ignores the information in the data about the address, and makes it impossible to rely on the natural structure of the address to help match the desired address with the input string. The latter approach is more complex, but flexible, it allows for more accurate comparison because comparing tokens precludes the possibility of the same word representing one element of the address being compared against an unrelated element of the address. 


### Candidate address retrieval
In the second step, we need a method to compare the input address with the reference data. The most common approach is to compare each token in the input address with each token in the reference data. This approach is simple, but it can't address the problem of typos and abbreviations. Alternatively, we can use a probabilistic data matching algorithm, such as Fuzzy string matching to accommodate for typos. For instance, “Birmingham ~ Birmingam” is one letter different (i.e. Levenshtein distance). However, in practice, when each record is compared against every other record, the number of comparisons can be very large, thus leading to a very expensive computation. Or instead, we can consider using a Natural Language Processing (NLP) approach to compare the addresses by assuming that the address is a sequence of unstructured text. One of the most common approaches is to convert the address into a vector and then compare the vector similarity (e.g., [TF-IDF](http://www.tfidf.com/)). 

### Scoring and ranking 

The last step in the process is to evaluate and convey quality of the match between the input and the retrieved candidate address in such a way that would be easily understood and useful for users. We can evaluate the quality of the match by comparing the input address with the candidate address. The most common approach is to use a similarity score to evaluate the similarity among all potential candidate addresses. Depending on the application, the similarity score can be a simple percentage or a more complex score. For instance, we can define a threshold for the similarity score, and only return the candidate addresses that have a similarity score above the threshold. We can also evaluate the model performance by validating the results with a ground truth dataset (e.g., address's unique property reference number (UPRN)). 

|    |   Account_Num | Facility Name                     | Address                   | City         | State   |   ZIP Code | County Name   | Phone Number   | Hospital Type             | Hospital Ownership             |
|---:|--------------:|:----------------------------------|:--------------------------|:-------------|:--------|-----------:|:--------------|:---------------|:--------------------------|:-------------------------------|
|  0 |         10605 | SAGE MEMORIAL HOSPITAL            | STATE ROUTE 264 SOUTH 191 | GANADO       | AZ      |      86505 | APACHE        | (928) 755-4541 | Critical Access Hospitals | Voluntary non-profit - Private |
|  1 |         24250 | WOODRIDGE BEHAVIORAL CENTER       | 600 NORTH 7TH STREET      | WEST MEMPHIS | AR      |      72301 | CRITTENDEN    | (870) 394-4113 | Psychiatric               | Proprietary                    |

Table 2 - Sample of the account hospital data


|    |   Provider_Num | Provider Name                    | Provider Street Address    | Provider City   | Provider State   |   Provider Zip Code |   Total Discharges |   Average Covered Charges |   Average Total Payments |   Average Medicare Payments |
|---:|---------------:|:---------------------------------|:---------------------------|:----------------|:-----------------|--------------------:|-------------------:|--------------------------:|-------------------------:|----------------------------:|
|  0 |         839987 | SOUTHEAST ALABAMA MEDICAL CENTER | 1108 ROSS CLARK CIRCLE     | DOTHAN          | AL               |               36301 |                118 |                   20855.6 |                  5026.19 |                     4115.52 |
|  1 |         519118 | MARSHALL MEDICAL CENTER SOUTH    | 2505 U S HIGHWAY 431 NORTH | BOAZ            | AL               |               35957 |                 43 |                   13289.1 |                  5413.63 |                     4490.93 | 


Table 3 - Sample of the reimbursement data 
## Considerations for GPU 

The address matching problem is a computationally intensive problem. The complexity of the problem is compounded by the fact that data is growing at an exponential rate. A core challenge is to understand which part of the process is the most computationally intensive and which part of the process can be parallelized on the GPU.

In the first part, we can concatenate the address from its constituent parts, whereby treating each address as a sequence of unstructured text. In the second part, we can use a character-based n-gram TF-IDF to convert the address into a vector. While the terms in TF-IDF are usually words, this is not a requirement. We can use n-grams: sequences of N continuous characters, to convert the address into a vector representation based on the character level. In the third part, we can use a similarity score to evaluate the similarity among all potential candidate addresses. In this case, we rank the candidate addresses based on the similarity score. 

Using TF-IDF with N-Grams as terms to find similar strings transforms the problem into a matrix multiplication problem, which is computationally much cheaper. This approach can significantly reduce the **memory** it takes to compare strings, as compared to the fuzzy string matching algorithm with TF-IDF and nearest neighbors algorithm. Furthermore, using a GPU for matrix multiplication can further enhance the **speed of comparison**. The greater the Levenshtein distance, the more different the strings are. By using a GPU, we can reduce the time it takes to calculate the Levenshtein distance, making the comparison of strings much faster.



An example of character-based n-gram TF-IDF
```python
text = "Birmingham" 

# Create a list of n-grams
n_grams = [text[i:i+3] for i in range(len(text)-2+1)]

print(n_grams)
#['Bir', 'irm', 'rmi', 'min', 'ing', 'ngh', 'gha', 'ham']

```

## Data 
In this tutorial, we will be using US hospital data. In these examples, we have two data sets. The [first](https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_account_info.csv) is an internal data set that contains basic hospital account number, name and ownership information. The [second](https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_reimbursement.csv) data set contains hospital information (called provider) as well as the number of discharges and Medicare payment for a specific Heart Failure procedure. The goal is to match up the hospital reimbursement information with the first data so we have more information to analyze our hospital customers. In this instance, we have 5339 hospital accounts and 2697 hospitals with reimbursement information. 



## Step 1: Import Libraries and Data pre-processing 

For the first step, let's compare the speed of computing TF-IDF using Sklearn and RAPIDS cuML module. To make the difference more distinguishable, let's test how the increase of vocabulary size would affect the performance in multiple runs.

```python
# Import libraries 
import cudf
import pandas as pd
from cuml.feature_extraction.text import TfidfVectorizer as GPU_TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer as CPU_TfidfVectorizer
import time 
import numpy as np
import seaborn as sns 
import matplotlib.pyplot as plt

# Import data
data1_url = "https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_account_info.csv"
data2_url = "https://raw.githubusercontent.com/chris1610/pbpython/master/data/hospital_reimbursement.csv"

account = pd.read_csv(data1_url) #Hospital account information
reimbursement = pd.read_csv(data2_url) #Hospital reimbursement information

#Converting facility name, address, city and state into one string 
account_full_address = account.apply(lambda x: " ".join([x['Facility Name'],x['Address'],x['City'],x['State']]), axis=1).tolist() 

reimbursement_full_address = reimbursement.apply(lambda x: " ".join([x['Provider Name'],x['Provider Street Address'],x['Provider City'],x['Provider State']]), axis=1).tolist() 

```

## Step 2: TF-IDF Vectorization - experimenting the effect of data size on computational time

In this step, we will be experimenting the effect of data size on computational time. We will be using the same data set, but we will be increasing the size of the data set by x times in each run. We will be using the same vocabulary size for each run. 

```python
#Inititate sklearn vectoriser and cuml vectosier 

#CPU vectorizer from sklearn 
cpu_vectorizer = CPU_TfidfVectorizer(analyzer='char',ngram_range=(1,2))
#GPU vectorizer from cuml 
gpu_vectorizer = GPU_TfidfVectorizer(analyzer='char',ngram_range=(1,2))

#Here analyzer='char' means we are using character as the input 
#ngram_range = (1,2) means we are looking at both unigram and bigram for the model input 


#Manually inflating number of rows with 10 run times 
total_datasize = []
cpu_time = []
gpu_time = [] 
for run in range(1,10):
  for i in range(1,50):
    #Manually inflating the number of records 
    input = reimbursement_full_address*i
    total_datasize.append(len(input))
    #Cpu runtime --------------------------------  
    start = time.time()
    cpu_output = cpu_vectorizer.fit_transform(input)
    done = time.time()
    elapsed = done - start 
    cpu_time.append(elapsed)

    #gpu runtime -------------------------------- 
    start = time.time()
    #Convert input to cudf series 
    gpu_output = gpu_vectorizer.fit_transform(cudf.Series(input))
    done = time.time()
    elapsed = done - start 
    gpu_time.append(elapsed)

#Create a dataframe to store the results
gpu_elapsed = pd.DataFrame({"time":gpu_time,"data_size":total_datasize,'label':"gpu"})
cpu_elapsed = pd.DataFrame({"time":cpu_time,"data_size":total_datasize,'label':"cpu"})
result = pd.concat([gpu_elapsed,cpu_elapsed]).reset_index()

fig, ax = plt.subplots(figsize=(10,10))
sns.lineplot(x= 'data_size',y='time',hue = 'label',data = result,ax = ax )
plt.xlabel('Data Size')
plt.ylabel("Time Elapsed ")
plt.title("Comparing the speed of TF-IDF vectorisation on CPU and GPU")
plt.show()
print(cpu_output.shape) #(25000, 874) -> meaning we have 24273 rows of address and 874 characters in the vocabulary as input 
```


<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/tf_idf_vec.png?raw=true">
    <figcaption>
    <b>Figure 1: Comparing the speed of TF-IDF vectorisation on CPU and GPU. 
    </b> 
    </figcaption>
    </center>
</figure>

There are few things we can observe the code above. Firstly, the code between the CPU and GPU is almost identical. The only difference is that we are using the functions from different libraries. Secondly, we can see that the GPU is able to process the data much faster than the CPU. On average, it takes less than 0.1 seconds to process around 25 thousand addresses on the GPU. In comparison, it takes 1 seconds on CPUs for the same input dataset. Furthermore, we can also observe that the run time of CuML’s TfidfVectorizer is almost constant as the input data size increases, whereas the run time of scikit-learn’s TfidfVectorizer exponetiates. This is because the GPU is able to process the data in parallel, which makes it much more efficient than the CPU. Lastly, 


Now that we have our TF-IDF matrix, we want to use the hospital reimbursement data to find the most relevant address from the hospital data. To do this, we can find the address with the smallest distance (or highest similarity) to our query address. For the second step, let's compare the speed of computing cosine similarity on GPU and CPU. 

## Step 3: Cosine Similarity - experimenting the effect of matrix multiplication on computational time

In this step, we will compare the computation time of calculating the pair-wise cosine similarity between two matrices using Numpy, Cupy and Numba from scratch. We will use the TF-IDF matrix we created from the reimbursement data in the previous step as the input, and hospital data as the target.  
 

```python
#Vectorize the target address
cpu_target =cpu_vectorizer.transform(account_full_address)
gpu_target = gpu_vectorizer.transform(cudf.Series(account_full_address))
```

```python
import cupy as cp
import numpy as np 
import scipy
import gc
import torch 
from cupyx.scipy.sparse.linalg import norm as cp_norm
from scipy.sparse.linalg import norm as sc_norm 
#Numpy --------------------------------------------------------
def np_cosine_similarity(query, target):
    # Assert that the input matrices have the same number of columns
    assert(query.shape[1] == target.shape[1])

    #Calculate the dot product 
    dot_product = np.dot(query,target.T)

    #Calculate l2 norm for query and target 
    query_norm = sc_norm(query,axis=1)
    target_norm = sc_norm(target,axis=1)

    return dot_product/(query_norm[:,np.newaxis]*target_norm[:,np.newaxis].T)

#Cupy --------------------------------------------------------
def cp_cosine_similarity(query, target):
    # Assert that the input matrices have the same number of columns
    assert(query.shape[1] == target.shape[1])
    #Initiate GPU instance 
    with cp.cuda.Device(0):
        #Check whether the sparse matrix is compatible with Cupy, if not then convert
        if isinstance(query,scipy.sparse._csr.csr_matrix) and isinstance(target,scipy.sparse._csr.csr_matrix):
        # Convert the input matrices to sparse format and copy to the GPU
            query = cp.sparse.csr_matrix(query, copy=True) 
            target = cp.sparse.csr_matrix(target, copy=True)
        
        # Dot product using cupy.dot()
        dot_product = query.dot(target.T)
        
        # Calculate l2 norm for query and target
        query_norm = cp_norm(query,axis=1)
        target_norm = cp_norm(target,axis=1)
        
        # Compute the cosine similarity
        output = dot_product / (cp.expand_dims(query_norm,axis=1) * cp.expand_dims(target_norm,axis=0))
        
        #Converting back to numpy array
        result = output.get()
    return result
``` 

From the above code, we can see that the differences between NumPy and CuPy are minimal. Most mathematics functions in CuPy such as calculating the dot products and l2 norm have the same API as NumPy and SciPy. However, there are few things worth noticing. Firstly, when using CuPy, we have to manually Initiate the GPU instance by calling function ``` with cp.cuda.Device(0): ```  and then execute your calculation. This allows to temporarily switching the currently active GPU device. Secondly, we need to manually specify the data type and transfer the data on the GPU instance by using ```cp.sparse.csr_matrix(...,copy =True) ```. This is to ensure we can properly prepare the data types for CUDA operation (for more information on the supporting data types, [see](https://docs.cupy.dev/en/stable/overview.html)). And lastly, to get the data from the GPU, we need to use ```.get()``` to return a copy of the array on host memory.

To go a step further, we can explicitly write computing kernels with Numba. Numba is a Just-In-Time (JIT) compiler for Python that can take functions with numerical values or arrays and compile them to assembly, so they run at high speed. One advantage of writing with Numba, is that we can write the low-level code in Python and then integrate it with CUDA assembly. In contrast to CuPy, Numba offers some flexibility in terms of the data types and kernels that can be used. However, Numba does not yet implement the full CUDA API, so some features are not available. It is worth noting here the goal of this post is to demonstrate the use of Numba, and not to provide a comprehensive guide on how to use Numba. If you are interested in learning more about Numba, [see](https://numba.pydata.org/).

The following codes are the implementation of the cosine similarity function using Numba. 


```python
#Numba ----------------------------------------------------------------
from numba import cuda 

#Define the kernel for calculation 
@cuda.jit #compiler decorator 
def pairwise_cosine_similarity(A, B, C):
    i, j = cuda.grid(2) #use 1 for x 
    if i < C.shape[0] and j < C.shape[1]:
        dot = 0.0
        normA = 0.0
        normB = 0.0
        for k in range(A.shape[1]):
            a = A[i, k]
            b = B[j, k]
            dot += a * b
            normA += a ** 2
            normB += b ** 2
        C[i, j] = dot / (normA * normB)**0.5

def Numba_cuda_cosine_similarity(query,target): 
     # Assert that the input matrices have the same number of columns
    assert(query.shape[1] == target.shape[1])
    ## Allocate memory on the device for the result
    output = cuda.device_array((query.shape[0],target.shape[0]))


    #Check whether the sparse matrix is compatible with numba, if not then convert
    if isinstance(query,scipy.sparse._csr.csr_matrix) and isinstance(target,scipy.sparse._csr.csr_matrix):
    # Convert the input matrices to numpy array and copy to the GPU
        query = cuda.to_device(query.toarray())
        target = cuda.to_device(target.toarray()) 
    #Set the number of threads in a block ----- 
    threadsperblock = (32,32)

    # Calculate the number of thread blocks in the grid 
    blockspergrid_x = (output.shape[0] + (threadsperblock[0] - 1)) // threadsperblock[0]
    blockspergrid_y = (output.shape[1] + (threadsperblock[1] - 1)) // threadsperblock[1]
    blockspergrid = (blockspergrid_x, blockspergrid_y)

    #Starting the kernel 
    pairwise_cosine_similarity[blockspergrid, threadsperblock](query, target, output)

    # Copy the result back to the host
    return output.copy_to_host()
```

Let's break down the code. ```@cuda.jit``` is a decorator, and it defines functions which will compile on the GPU (kernels). The followed gunction ```pairwise_cosine_similarity```  defines the steps of calculation inside the kernel. The ```cuda.grid``` function returns the 2D grid indices for the current thread executing in a CUDA kernel. The CUDA programming model uses a grid of threads to perform parallel computation, where each thread operates on a different portion of the input data. In the example code, the ```cuda.grid``` function is used to determine the indices of the current thread in the grid, represented as ```i``` and ```j``` in the code. These indices are used to access the corresponding row of the input matrices ```A``` and ```B```, and the corresponding location in the output matrix ```C```. In essence, the ```cuda.grid``` function is used to ensure that each thread operates on a different portion of the input data and outputs its result to the corresponding location in the output matrix, thus enabling the parallel computation of the cosine similarity. Furthermore, we need to manually define the shape of matrix ```C```, and allocate memory on the device for the result. This is because the kernels cannot return numerical values, so we can get around that by passing inputs and outputs.  

In the ```Numba_cuda_cosine_similarity``` function, we are following the same logic in the CuPy function, however, we have to manually define the number of threads in a block and calculate the number of thread blocks in the grid. The ```threadsperblock``` defines a group of threads that can execute concurrently. A single thread represents the smallest unit of execution in a CUDA kernel. Each thread operates on a different portion of the input data and computes its result. The results computed by each thread are then combined to produce the final result. In general, the number of threads per block is a trade-off between performance and memory usage. Increasing the number of threads per block can increase performance by allowing more parallel computation, but it also increases the memory usage of the block. The GPU has a limited amount of shared memory that is shared by all threads in a block, and increasing the number of threads per block can cause the shared memory usage to exceed the available memory. The value of ```32, 32``` for ```threadsperblock``` is a common choice because it is a relatively small number of threads that can still provide good performance, while minimizing the memory usage of the block. However, the optimal value for ```threadsperblock``` depends on the specific requirements of the computation and the hardware being used, and it may need to be adjusted for different computations or hardware. 


Next we define the number of thread blocks in the grid with the variable ```blockspergrid``` . The number of thread blocks in the grid is defined by the number of threads in each dimension of the block, and the number of rows and columns in the output matrix. The purpose of adding ```(threadsperblock[0] - 1) to C.shape[0]``` is to ensure that the integer division will round up to the nearest whole number, rather than rounding down. In the last step, we can invoke the kernel by the defined block size and thread size and passing the input matrices and the output matrix to the kernel function. The kernel function will then perform the computation in parallel on the GPU. After the computation is finished, we can copy the result back to the host. 

```python

total_datasize = []
cpu_time = []
gpu_time = [] 
numba_time = [] 

for size in tqdm(range(1000,cpu_output.shape[0],5000)):
    total_datasize.append(size)
    query = cpu_output[0:size,]
    #CPU --------------
    start = time.time()
    _ = np_cosine_similarity(query,cpu_target)
    done = time.time()
    elapsed = done - start 
    cpu_time.append(elapsed)
    
    #GPU --------------
    start = time.time()
    _ = cp_cosine_similarity(query,cpu_target)
    done = time.time()
    elapsed = done - start 
    gpu_time.append(elapsed)

    #Numba CUDA ---- 
    start = time.time()
    _ = Numba_cuda_cosine_similarity(query,cpu_target)
    done = time.time()
    elapsed = done - start 
    numba_time.append(elapsed) 

fig, ax = plt.subplots(figsize=(10,10))
gpu_elapsed = pd.DataFrame({"time":gpu_time,"data_size":total_datasize,'label':"gpu"})
cpu_elapsed = pd.DataFrame({"time":cpu_time,"data_size":total_datasize,'label':"cpu"})
numba_elasped = pd.DataFrame({"time":numba_time,"data_size":total_datasize,'label':"numba"}) 
result = pd.concat([gpu_elapsed,cpu_elapsed,numba_elasped]).reset_index()
sns.lineplot(x= 'data_size',y='time',hue = 'label',data = result,ax = ax )
plt.xlabel('Data Size')
plt.ylabel("Time Elapsed ")
plt.title("Comparing the speed of matrix multiplication on CPU and GPU")
plt.show()
```


<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp1_matrix_multiplication.png?raw=true">
    <figcaption>
    <b>Figure 2: Comparing the speed of matrix multiplication on CPU and GPU. 
    </b> 
    </figcaption>
    </center>
</figure>

Again we can see a major speed difference as well. Overall, the operations on GPU are much faster than the ones on CPU. Specifically, leveraging JIT compilation and Numba CUDA, we can reduce the time of matrix multiplication from 10.8 seconds to around 1 seconds on comparing 24273x874 matrix with 5339x87 matrix. The code above is a demonstration of matrix multiplication, but in practice, we can use functions such as ```cosine_similarity``` from ```sklearn.metrics.pairwise``` or ```sparse_pairwise_distances ```from ```cuml.metrics``` to calculate the cosine similarity between two matrices.
 
## Step 4: Sanity Check 

Let's see how well the model perform in our data linkage task. In this step, we will use the address with the highest cosine similarity result to find the best match for each query. Since we don't have the unique identifier for each address, we will be using the zip code to check the accuracy of the model.   



```python 
from collections import defaultdict 
#Full - pipeline on GPU 
reimbursement_tfidf= gpu_vectorizer.fit_transform(cudf.Series(reimbursement_full_address))
account_tfidf = gpu_vectorizer.transform(cudf.Series(account_full_address))
similarity_matrix = Numba_cuda_cosine_similarity(reimbursement_tfidf,account_tfidf) 

result = defaultdict(list)
#Getting the reimbursement address and state, account address and state 
for index in range(similarity_matrix.shape[0]):
    most_similar_index = similarity_matrix[index].argmax()
    result['reimbursement_address'].append(reimbursement_full_address[index]) 
    result['account_address'].append(account_full_address[most_similar_index])  
    result['reimbursment_zip'].append(reimbursement.loc[index,'Provider State'])
    result['account_zip'].append(account.loc[most_similar_index,'State']) 
    result['similarity_score'].append(similarity_matrix[index][most_similar_index])

result_df = pd.DataFrame(result)
print(result_df.sort_values('similarity_score',ascending=True).head(2) 
#print(result_df.sort_values('similarity_score',ascending=False).head(2) 
``` 

From the tables below, we can see that the matched records with the lowest cosine similarity score are not the same, but the matched records with the highest cosine similarity score are the almost identical. This is a good sign that the model is working as expected. 



|      | reimbursement_address                            | account_address                                                                |   reimbursment_zip |   account_zip |   similarity_score |
|-----:|:-------------------------------------------------|:-------------------------------------------------------------------------------|-------------------:|--------------:|-------------------:|
| 1325 | UNITY HOSPITAL 550 OSBORNE ROAD FRIDLEY MN       | UNITY HOSPITAL OF ROCHESTER 1555 LONG POND ROAD ROCHESTER NY                   |              55432 |         14626 |           0.551186 |
| 1634 | TLC HEALTH NETWORK 845 ROUTES 5 AND 20 IRVING NY | STANDING ROCK INDIAN HEALTH SERVICE HOSPITAL 10 NORTH RIVER ROAD FORT YATES ND |              14081 |         58538 |           0.555283 |

Table 1: Examples of matched records with the lowest cosine similarity score.



|      | reimbursement_address                                                         | account_address                                                               |   reimbursment_zip |   account_zip |   similarity_score |
|-----:|:------------------------------------------------------------------------------|:------------------------------------------------------------------------------|-------------------:|--------------:|-------------------:|
| 2242 | BAPTIST MEMORIAL HOSPITAL UNION CITY 1201 BISHOP ST, PO BOX 310 UNION CITY TN | BAPTIST MEMORIAL HOSPITAL UNION CITY 1201 BISHOP ST, PO BOX 310 UNION CITY TN |              38261 |         38261 |                  1 |
|   89 | BANNER DESERT MEDICAL CENTER 1400 SOUTH  DOBSON ROAD MESA AZ                  | BANNER DESERT MEDICAL CENTER 1400 SOUTH  DOBSON ROAD MESA AZ                  |              85202 |         85202 |                  1 | 

Table 2: Examples of matched records with the highest cosine similarity score.



```python 
from sklearn.metrics import accuracy_score
print(accuracy_score(result_df.reimbursment_zip,result_df.account_zip)) #0.97
``` 

The accuracy score is 0.97, which 97% of the time, the model can find the best match for each reimbursement hospital with identical zip code. 

Furthermore, by inspecting the distribution of similarity score, we can see that majority of matched records have cosine similarity score above 0.95. However, there are some records that have a similarity score below 0.5, this maybe due to the absence of such address from the hospital account holders.

```python
import matplotlib.pyplot as plt
fig,ax = plt.subplots(figsize=(10,10))
result_df.similarity_score.plot(kind='hist',bins=10,ax=ax)
plt.xlabel('Cosine Similarity Score')
plt.title('Distribution of cosine similarity score') 
```

<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/chp1_cosine_distribution.png?raw=true">
    <figcaption>
    <b>Figure 3: Distribution of cosine similarity score among matched records. 
    </b> 
    </figcaption>
    </center>
</figure>



## Conclusion

In this post, we have explored the potential of utilizing GPUs to speed up the data linkage process. By implementing vectorization and cosine similarity calculation on a GPU, we were able to significantly enhance the performance of the process. More importantly, we can see that the functions and code that we have written can be easily ported to a GPU without much modification. 

However, address matching is a complex problem that involves many factors that can impact the accuracy of the model. Some of these challenges include the presence of special characters, abbreviations, changes and absence of unique identifiers in the data. Often the task of data linkage is not a one-time process, but rather an iterative process that requires constant monitoring on the performance of the model and making adjustments accordingly. GPU can be a great tool to help speed up the numerical processes, but it's important to note that it's not a silver bullet that can solve all the problems.

Furthermore, when dealing with large datasets, we have to be aware of the memory limitation of the GPU. One might consider using a GPU in conjunction with a scaling tool such as DASK, which can distribute the computation across multiple machines and servers. This approach can help mitigate the GPU memory limitation that can arise when processing larger datasets. Additionally, we can consider the batch size or the number of records that can be processed at a time to ensure that the GPU's memory is not overloaded. 

