---
title: "Chapter 1 - Address matching and Geocoding address"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

## Addresses and Geocoding textual datasets using GPU 

Address matching is an example of a broad class of data science problems known as data linkage. The common aspect of all these problems is that they require an association between data (commonly two sources) to be devised based on some common property of the data. For address matching, the common property is the address itself but, given that there is no universally accepted standard for the “correct” form of the address, and sources commonly store address data in many different forms, matching addresses becomes a significantly complicated task that requires a great deal of data preparation and processing. Table 1 provides a summary of the linkage problem:



|Input address string| Reference data to match against| 
| --- | ----------- |
|Unstructured text |Structured, tokenised|
|Messy, containing typos and abbreviations|Clean, standardised, correct (In most cases)| 
|Incomplete| Snapshot of addresses at a given time| 
|Range from historic to very recent addresses, including businesses| Organisation / business names are not always part of the address| 

Table 1: Summary of the address matching problem, [Office for National Statistics](https://www.ons.gov.uk/methodology/methodologicalpublications/generalmethodology/onsworkingpaperseries/onsworkingpaperseriesno17usingdatasciencefortheaddressmatchingservice#:~:text=Address%20matching%20is%20an%20example,common%20property%20of%20the%20data.) 


As we can see, the address matching problem is a very complex problem, the complexity of which is compounded by the fact that data is growing at an exponential rate. Therefore, the objective of this chapter is to demonstrate how to use GPU to solve the address matching. 

## Address matching pipeline 

The address matching process can be split into three high-level steps:

### Data pre-processing 
At the most fundamental level, we need to prepare the data for the matching process. There are potentially different approach to do this, but the most common approach is to concatenate (join) the address into its constituent parts. Alternatively, the input address can be split into corresponding parts, such as the street name, house number, postcode, and so on. The former approach is more common, but it ignores the information in the data about the address, and makes it impossible to rely on the natural structure of the address to help match the desired address with the input string. The latter approach is more complex, but flexible, it allows for more accurate comparison because comparing tokens precludes the possibility of the same word representing one element of the address being compared against an unrelated element of the address. 



At the most foundemntal level, the goal of data linkage is to compute the similarity between two strings (e.g. Levenshtein, Damerau-Levenshtein, Jaro-Winkler, q-gram, cosine).

One approach we can experiement is to transform each string into some kind of vectors and compute the pair-wise similarity. 

- Load our data 
- Vectorize our data using TF-IDF 
- Compare pair-wise similairty of the data

At each steps, we can compare the compuatation speed differences between PAPIDS cuML and Sklearn.  


## Data 
In this tutorial, I have two data sets. The first is a data set that contains basic hospital account number, name and ownership information. The second data set contains hospital information (called provider) as well as the number of discharges and Medicare payment for a specific Heart Failure procedure. The goal is to match up the hospital reimbursement information with the first data so we have more information to analyze our hospital customers. In this instance, we have 5339 hospital accounts and 2697 hospitals with reimbursement information. 
