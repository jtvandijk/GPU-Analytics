---
title: "Setting up the environment for GPU"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

## Introduction 
Setting up the environment for GPU can be challenging, as every computer has different hardware and software configurations. There are no universal instructions that will work for everyone, but in this chapter, we will discuss how to set up the environment for GPU in local-host and Google Colab. We will also highlight the steps to verify that the GPU is working properly. 


## Setting up the environment for Google Colab GPU 

Google Colab is a cloud-based platform that allows you to run Jupyter notebooks in the cloud, with support for running on GPUs. Google Colab already pre-installed libraries such as pandas or NumPy, so you do not need to run “pip install” by yourself. Here are the steps to set up a GPU for use in Colab:

### How to Start? 

The first step is to create a new notebook in Google Colab. You can do this by going to the (Google Colab website)[https://colab.research.google.com/], signing in with your Google account, and creating a new notebook.  

<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/Colab_create.png?raw=true">
    <figcaption>
    <b>Figure 1: Create a new notebook in Google Colab. 
    </b> 
    </figcaption>
    </center>
</figure>


### Enabling GPU support in Google Colab 

After creating a new notebook, you need to enable GPU support. To do this, go to the "Runtime" menu and select "Change runtime type." Then, set the "Hardware accelerator" to "GPU." 


<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/Colab_run_time.png?raw=true">
    <figcaption>
    <b>Figure 2: Select "Change runtime type" in the "Runtime" menu.
    </b> 
    </figcaption>
    </center>
</figure>

After you change your runtime type, your notebook will restart, which means information from the previous session will be lost. 


<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/Colab_setting.png?raw=true">
    <figcaption>
    <b>Figure 3: Select "GPU" in the "Hardware accelerator" menu. 
    </b> 
    </figcaption>
    </center>
</figure>

You may also see the TPU option in the drop-down menu. TPU stands for a Tensor Processing Unit, which is a specialized chip designed to accelerate machine learning workloads. TPU is more powerful than GPU, but it is also more expensive.

### Verify GPU access 

After you enable GPU support, you can verify that the GPU is working properly by running the following code in a code block in your notebook. **nvidia-smi** (NVIDIA System Management Interface) is a tool to query, monitor and configure NVIDIA GPUs. It ships with and is installed along with the NVIDIA driver and it is tied to that specific driver version. It is a tool written using the NVIDIA Management Library (NVML), which you can also find the Python bindings in the PyPI package pynvml. 

```Python
!nvidia-smi
```

<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/Colab_smi.png?raw=true">
    <figcaption>
    <b>Figure 4: NVIDIA System Management Interface (nvidia-smi) output.
    </b> 
    </figcaption>
    </center>
</figure>

This outputs a summary table, where I find the following information useful:
- **Name**: The name of the GPU.
- **Memory**: The amount of memory available on the GPU.
- **Utilization**: The percentage of time the GPU is being used.
- **Power usage**: The power usage of the GPU.
- **Processes**: List of processes executing on the GPUs.

In this example, we got a Tesla T4 GPU with 16GB of memory. The GPU is currently being used by the notebook, but we can also see that there is no other process running on the GPU.


There are different ways to verify that the GPU is working properly. For example, you can run the following code to check the name or the presence of the GPU 

```Python
import torch
## Check whether GPU is available
torch.cuda.is_available()
## Query the name of the GPU
torch.cuda.get_device_name(0)
```

Just like your own notebook, you can also install packages in Colab.  

```Python
!pip install gputil
!pip install psutil
!pip install humanize
```

To access the memory usage on CPU and GPU, we can use a simple Python script. 

```Python
# Import packages
import os,sys,humanize,psutil,GPUtil

# Define function
def mem_report():
  print("CPU RAM Free: " + humanize.naturalsize( psutil.virtual_memory().available ))
  
  GPUs = GPUtil.getGPUs()
  for i, gpu in enumerate(GPUs):
    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))
    
# Execute function
mem_report()
```

The output of the above code is as follows:

```Python
CPU RAM Free: 12.8 GB
GPU 0 ... Mem Free: 15109MB / 16130MB | Utilization  10%
```



### Import and Installing the libraries 

```Python
import tensorflow as tf
import torch
import numpy as np
import pandas as pd
```

Check GPU's usage in the Colab, there is a option in the top right corner to check the GPU usage.

And, now you are ready to use your GPU power in the Colab, you can run all the deep learning codes and it will use the GPU power.
It's worth mentioning that Colab is also providing TPU, Tensor Processing Unit for more computational power.



## Setting up the environment for local-host GPU 

## Install NVDIA CUDA Toolkit and drivers 

