---
title: "GPU"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

## Introduction 

[Add reference to the book chapter here ]

Over the past few decades, social science research has faced challenges due to a lack of available data, the cost and time needed to conduct surveys, and limitations on computational power for analysis. These issues were exacerbated by declining survey quality and biases in the characteristics of respondents. There was also no guarantee that long-term, time-consuming surveys would continue to be available due to fiscal austerity (Singleton et al., 2017). However, in recent years, the availability of big data has greatly expanded the resources available to social scientists, allowing them to access a wide range of information about individuals and societies from sources such as social media, mobile devices, and online transactions. The rapid expansion of data is transforming the practice of social science, providing increasingly granular spatial and behavioural profiles about the society and individuals. The difference is not just a matter of scale. The increasing data availability extends beyond the realm of identifying consumer preferences, allowing us to measure social processes and the dynamics of spatial trends in unprecedented detail. 

For example, Trasberg and Cheshire (2021) used a large scale mobility data from mobile applications to explore the activity patterns in London during lockdown, identifying the socio-spatial fragmentation between urban communities. Similarly, Van Dijk (2020) used an annually updated Consumer Register (LCR) to estimate residential moves and the socio-spatial characteristics of the sedentary population in the UK, overcoming the limitations of the traditional census data. Gebru et al. (2020), on the other hand, used a large scale dataset of images from Google Street View to estimate the demographic makeup of a neighborhood. Their results suggested a possibility of using automated systems for monitoring demographics may effectively complement labor-intensive approaches, with the potential to measure demographics with fine spatial resolution, in close to real time.

With the growing availability of data, processing and analyzing large datasets need more computational power than is currently available, with more complex algorithms that need more compute power to run. To overcome this challenge, researchers have turned to a GPU (Graphics Processing Unit) to accelerate massive data parallelism and computation. Therefore, in this chapter, we will introduce the concept of GPU and explain why GPU can be a useful tool for social scientists. In addition, we will provide a brief introduction to the CUDA and RAPIDS package, which is a GPU-accelerated framework that can be used to accelerate the data analysis process. 

## [CPU vs GPU](https://developer.nvidia.com/blog/cuda-refresher-reviewing-the-origins-of-gpu-computing/)

A CPU is a general-purpose processor that is capable of executing a wide range of tasks, including running operating systems, executing programs, and performing calculations. CPUs are typically designed with a focus on sequential processing, meaning they are optimized for executing instructions one at a time in a specific order. They have a relatively small number of processing cores (usually between 2 and 16) and are capable of performing a wide range of functions.

A GPU, on the other hand, is a specialized processor that is designed specifically for handling graphics and visualizations. GPUs have numerous processing cores (usually hundreds or thousands) and are optimized for parallel processing, meaning they can perform many calculations simultaneously. This makes them particularly well-suited for tasks that require a lot of processing power, such as rendering 3D graphics, running machine learning algorithms, or performing scientific simulations.

The main difference between CPU and GPU is the designed architecture (Figure 1). GPUs dedicate most of their transistors for ALU units (Arithmetic Logic Unit) which are responsible for performing arithmetic and logical operations, while CPUs reserve most of their transistors for caches and control units which aim to reduce latency within each thread. 

Most CPUs are multi-core processors, meaning they have multiple processing cores that can execute instructions with Multiple data streams. This architecture is called Multiple Instruction, Multiple Data (MIMD) - They are designed to minimize the time it takes to access data (white bars in Figure 2). In a single time slice, a CPU thread tries to get as much work done as possible (green bar). To achieve this, CPUs require low latency, which is achieved through large caches and complex control logic. However, caches work best with only a few threads per core, as switching between threads is expensive. 

GPUs, on the other hand, hide instruction and memory latency with computation. GPUs use a Single Instruction, Multiple Data (SIMD) architecture, where each thread is assigned a small amount of memory (blue bar), resulting a much higher latency per thread. However, GPUs have many threads per core, and it can switch from one thread to another at no cost, resulting higher throughput and bandwidth for large data. What this means, in the end, is that we can store more data in the GPU memory and caches, which can be reused for matrix multiplication and operations that are more computationally intensive.

As shown in Figure 2, when thread T1 is waiting for data, another thread T2 begins processing, and so on with T3 and T4. In the meantime, T1 eventually gets the data it needs to process. In this way, latency is hidden by switching to other available work. As a result, GPUs can utilize overlapping concurrent threads to hide latency and are able to run thousands of threads at once.  The best CPUs have about 50GB/s while the best GPUs have 750GB/s memory bandwidth. So the larger your computational operations are in terms of memory, the larger the advantage of GPUs over CPUs. 

We can make the minions' analogy to explain the difference between CPU and GPU. A CPU is like Gru who is considerably intelligent and capable of building fantastic machines, but he can only do one thing at a time. A GPU is like a swarm of minions who are not as intelligent as Gru, but they can do build one thing collectively.


<figure title = "test">
    <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/GPU_CPU.png?raw=true">
    <figcaption>
    <b>Figure 1: CPU and GPU archetecture. CPU devote more transistors to control data flow, while GPUs devote more transistors to compute data processing.
    </b> 
    </figcaption>
    <center>
</figure>


<figure title = "test">
    <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/GPU_CPU_process.png?raw=true">
    <figcaption>
    <b>Figure 2: CPU and GPU processor architecture.
    </b> 
    </figcaption>
    <center>
</figure>





## Is GPU better than CPU?

It is not accurate to say that one is always better than the other, as both CPUs and GPUs have their own strengths and are optimized for different types of tasks - not all tasks can be accelerated by a GPU. 

<figure title = "test">
    <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/Dimension_of_scale.png?raw=true">
    <figcaption>
    <b>Figure 3: Dimension of scale.
    </b> 
    </figcaption>
    <center>
</figure>



First thing to consider is the scale of the data. A good GPU can read/write its memory much faster than the host CPU can read/write its memory. GPUs can perform calculations much faster and more efficient to process massive datasets than the host CPU. An example can be observed in Figure 4 - 
<a href="https://www.mathworks.com/help/parallel-computing/measuring-gpu-performance.html">an example of matrix multiplication on a CPU and a GPU.</a>  As shown in the figure, the CPU can be faster when the matrix size is small, but the GPU is much faster when the matrix size is large. This also applies to other data science tasks such as neural network training. You may get away without a GPU if your neural network is small in scale. However, it might be worthwhile to consider investing in a GPU if the neural network involves hundreds of thousands of parameters and requires extensive training times. 

<figure title = "test">
    <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/CPU_GPU_speed.png?raw=true">
    <figcaption>
    <b>Figure 4: Computational speed between CPU and GPU.
    </b> 
    </figcaption>
    <center>
</figure>

The second thing to consider is the type of projects. If you are working on a project that requires a lot of parallel processing and calculation, such as rendering large spatial objects, running machine learning algorithms, or performing scientific simulations, a GPU is a good choice. However, if you are working on a project that requires a high level of sequential processing, such as running multiple functions in a loop, a CPU is a good choice that offer more flexibility.

The last thing to bear in mind is the accessibility. Access to GPUs is not as easy as CPUs. GPUs are usually more expensive than CPUs, and they are not as widely available. In general, if you are working with a <a href = "https://thechief.io/c/editorial/comparison-cloud-gpu-providers/"> Cloud GPU providers </a> such as Google Colab, Microsoft Azure, or Amazon AWS, the cost of GPU per hour would be somewhere between $0.5 to $3.5. If you are working with a local GPU, the average cost is around $450. Furthermore, you need to make sure that your GPU is compatible with the CUDA toolkit, which are compatible with the libraries that support GPU computing. M1 chips, for example, are not compatible with CUDA toolkit, prohibiting some GPU computing libraries from being used.

## Why GPU is becoming popular - [The NVIDIA® CUDA® Toolkit](https://developer.nvidia.com/blog/cuda-refresher-the-gpu-computing-ecosystem/)

For the past few years, GPUs have become more and more popular in data science. This is mainly due to the development of the CUDA and CUDA toolkit, which simplifies GPU-based programming. 

CUDA (**C**ompute **U**nified **D**evice **A**rchitecture) is a general purpose parallel computing platform and application programming interface (API) developed by NVIDIA. CUDA programming is a hybrid model where serial sections of code run on the CPU and parallel sections run on the GPU (see, Figure 5). CUDA gives us direct access to the GPU’s virtual instruction set and parallel computational elements, making it easier for compiling compute kernels into code that will execute efficiently on the GPU. In CUDA, the computation is divided into small units of work called threads. Threads are grouped into thread blocks, and multiple thread blocks can be executed simultaneously on the GPU. Thread blocks are further grouped into grids, which represent a collection of thread blocks that can be executed in parallel on the GPU.

In CUDA, the function (kernel) is exacuated with the aid of threads. Each thread performs the same operation on different elements of the input data, allowing the GPU to perform the computation in parallel. A block of threads is a controlled by the streaming multiprocessing unit that can be scheduled and executed concurrently. Several blocks of threads can be grouped into a grid, which constitutes the whole GPU unit. 

The relationship between grid, thread, and block in CUDA can be visualized as a three-dimensional structure, with each thread block representing a plane and each grid representing a collection of planes. The number of threads per block determines the width of the plane, and the number of blocks per grid determines the number of planes in the collection. The GPU schedules and executes the threads in the grid, and each thread performs its computation on a different element of the input data. 



<figure title = "test">
     <center>
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/GPU_software.png?raw=true">
    <figcaption>
    <b>Figure 5: How GPU acceleration works. 
    </b> 
    </figcaption>
    </center>
</figure>

Furthermore, CUDA tookit and many third-party libraries offers collection well-optimized and pre-compiled functions that enable drop-in acceleration across multiple domains such as linear algebra, image and video processing, deep learning, and graph analytics. For developing custom algorithms, you can use available integrations with commonly used languages and numerical packages, as well as well-published development API operations. Here is a list of some widely used libraries: 

- Mathematical libraries: cuBLAS, cuRAND, cuFFT, cuSPARSE, cuTENSOR, cuSOLVER
- Parallel algorithm libraries: nvGRAPH, Thrust
- Image and video libraries: nvJPEG, NPP, Optical Flow SDK
- Communication libraries: NVSHMEM, NCCL
- Deep learning libraries: cuDNN, TensorRT, Riva, DALI
- Partner libraries: OpenCV, FFmpeg, ArrayFire, MAGMA  

As a social scientist, you may not need to use all of these libraries. However, it is important to know that these libraries exist and are available for you to use. Libraries such as [RAPIDS](https://rapids.ai/), [Numba](https://numba.readthedocs.io/en/stable/user/5minguide.html) or [PyTorch](https://pytorch.org/) offer a high level programming interface that allows you to use GPU computing without having to learn C/C++ or CUDA. Furthermore, library such as RAPIDS cuDf and CuPy provide a drop-in replacement for Pandas and Numpy allowing you to use GPU computing for tasks such as data cleaning, feature engineering, and machine learning without having to change too much of your code. 

Here are some of the libraries that fall under the RAPIDS banner and their CPU-based counterparts (the full list of available APIs are provided [here](https://docs.rapids.ai/api)):

- cuDF (Pandas)
- cuML (Sklearn)
- cuGraph (NetworkX)
- cuSpatial (Geopandas/Shapely) 
- CuPy (Numpy)

It is important to note that CUDA is not the only parallel computing platform. With the rise of GPU computing, companies such as AMD and Intel have developed their own computing framework. [OpenCL](https://www.khronos.org/opencl/) is another parallel computing platform and programming model that is supported by AMD and Intel. However, CUDA is still the most popular parallel computing platform, and it is the only one that is supported by NVIDIA. 


## References 
.... 


