---
title: "GPU"
weight: 1
# bookFlatSection: false
# bookToc: true
# bookHidden: false
# bookCollapseSection: false
# bookComments: false
# bookSearchExclude: false
---

## Introduction 

Over the past few decades, social science research has faced challenges due to a lack of available data, the cost and time needed to conduct surveys, and limitations on computational power for analysis. These issues were exacerbated by declining survey quality and biases in the characteristics of respondents. There was also no guarantee that long-term, time-consuming surveys would continue to be available due to fiscal austerity (Singleton et al., 2017). However, in recent years, the availability of big data has greatly expanded the resources available to social scientists, allowing them to access a wide range of information about individuals and societies from sources such as social media, mobile devices, and online transactions. The rapid expansion of data is transforming the practice of social science, providing increasingly granular spatial and behavioural profiles about the society and individuals. The difference is not just a matter of scale. The increasing data availability extends beyond the realm of identifying consumer preferences, allowing us to measure social processes and the dynamics of spatial trends in unprecedented detail. 

For example, Trasberg and Cheshire (2021) used a large scale mobility data from mobile applications to explore the activity patterns in London during lockdown, identifying the socio-spatial fragmentation between urban communities. Similarly, Van Dijk (2020) used an annually updated Consumer Register (LCR) to estimate residential moves and the socio-spatial characteristics of the sedentary population in the UK, overcoming the limitations of the traditional census data. Gebru et al. (2020), on the other hand, used a large scale dataset of images from Google Street View to estimate the demographic makeup of a neighborhood. Their results suggested a possibility of using automated systems for monitoring demographics may effectively complement labor-intensive approaches, with the potential to measure demographics with fine spatial resolution, in close to real time.

With the growing availability of data, processing and analyzing large datasets need more computational power than is currently available, with more complex algorithms that need more compute power to run. To overcome this challenge, researchers have turned to a GPU (Graphics Processing Unit) to accelerate massive data parallelism and computation. Therefore, in this chapter, we will introduce the concept of GPU and explain why GPU can be a useful tool for social scientists. In addition, we will provide a brief introduction to the CUDA and RAPIDS package, which is a GPU-accelerated framework that can be used to accelerate the data analysis process. 

## [CPU vs GPU](https://developer.nvidia.com/blog/cuda-refresher-reviewing-the-origins-of-gpu-computing/)

A CPU is a general-purpose processor that is capable of executing a wide range of tasks, including running operating systems, executing programs, and performing calculations. CPUs are typically designed with a focus on sequential processing, meaning they are optimized for executing instructions one at a time in a specific order. They have a relatively small number of processing cores (usually between 2 and 16) and are capable of performing a wide range of functions.

A GPU, on the other hand, is a specialized processor that is designed specifically for handling graphics and visualizations. GPUs have a large number of processing cores (usually hundreds or thousands) and are optimized for parallel processing, meaning they can perform many calculations simultaneously. This makes them particularly well-suited for tasks that require a lot of processing power, such as rendering 3D graphics, running machine learning algorithms, or performing scientific simulations.

The main difference between CPU and GPU is the designed architecture (Figure 1). GPUs dedicate most of their transistors for ALU units(Arithmetic Logic Unit) which are responsible for performing arithmetic and logical operations, while CPUs reserve most of their transistors for caches and control units which aim to reduce latency within each thread. 

The way that CPUs and GPUs process data is different due to their architectural differences. CPUs are designed to minimize the time it takes to access data (white bars in Figure 2). In a single time slice, a CPU thread tries to get as much work done as possible (green bar). To achieve this, CPUs require low latency, which is achieved through large caches and complex control logic. However, caches work best with only a few threads per core, as switching between threads is expensive.

GPUs, on the other hand, hide instruction and memory latency with computation. In GPUs, each thread is assigned a small amount of memory (blue bar), resulting a much higher latency per thread. However, GPUs have many threads per core, and it can switch from one thread to another at no cost, resulting higher throughput and bandwidth for large data. What this means, in the end, is that we can store more data in the GPU memory and caches, which can be reused for matrix multiplication and operations that are more computationally intensive.

As shown in Figure 2, when thread T1 is waiting for data, another thread T2 begins processing, and so on with T3 and T4. In the meantime, T1 eventually gets the data it needs to process. In this way, latency is hidden by switching to other available work. As a result, GPUs can utilize overlapping concurrent threads to hide latency and are able to run thousands of threads at once.  The best CPUs have about 50GB/s while the best GPUs have 750GB/s memory bandwidth. So the larger your computational operations are in terms of memory, the larger the advantage of GPUs over CPUs. 

We can make the minions' analogy to explain the difference between CPU and GPU. A CPU is like Gru who is considerably intelligent and capable of building fantastic machines, but he can only do one thing at a time. A GPU is like a swarm of minions who are not as intelligent as Gru, but they can do build one thing collectively.


<figure title = "test">
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/GPU_CPU.png?raw=true">
    <figcaption>
    <b>Figure 1: CPU and GPU archetecture. CPU devote more transistors to control data flow, while GPUs devote more transistors to compute data processing.
    </b> 
    </figcaption>
</figure>


<figure title = "test">
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/GPU_CPU_process.png?raw=true">
    <figcaption>
    <b>Figure 2: CPU and GPU processor architecture.
    </b> 
    </figcaption>
</figure>




## Is GPU better than CPU?

It is not accurate to say that one is always better than the other, as both CPUs and GPUs have their own strengths and are optimized for different types of tasks - not all tasks can be accelerated by a GPU.

First thing to consider is the scale of the data. A good GPU can read/write its memory much faster than the host CPU can read/write its memory. Given large enough data, GPUs can perform calculations much faster than the host CPU. An example can be observed in Figure 3 - 
<a href="https://www.mathworks.com/help/parallel-computing/measuring-gpu-performance.html">an example of matrix multiplication on a CPU and a GPU.</a>  As shown in the figure, the CPU can be faster when the matrix size is small, but the GPU is much faster when the matrix size is large. This also applies to other data science tasks such as neural network training. You may get away without a GPU if your neural network is small in scale. However, it might be worthwhile to consider investing in a GPU if the neural network involves hundreds of thousands of parameters and requires extensive training times. 

<figure title = "test">
     <p><img src="https://github.com/jasoncpit/GPU-Analytics/blob/master/Pictures/CPU_GPU_speed.png?raw=true">
    <figcaption>
    <b>Figure 3: Computational speed between CPU and GPU.
    </b> 
    </figcaption>
</figure>

The second thing to consider is the type of projects. If you are working on a project that requires a lot of parallel processing and calculation, such as rendering large spatial objects, running machine learning algorithms, or performing scientific simulations, a GPU is a good choice. However, if you are working on a project that requires a high level of sequential processing, such as running multiple functions in a loop, a CPU is a good choice that offer more flexibility.

The last thing to bear in mind is the accessibility. Access to GPUs is not as easy as CPUs. GPUs are usually more expensive than CPUs, and they are not as widely available. In general, if you are working with a <a href = "https://thechief.io/c/editorial/comparison-cloud-gpu-providers/"> Cloud GPU providers </a> such as Google Colab, Microsoft Azure, or Amazon AWS, the cost of GPU per hour would be somewhere between $0.5 to $3.5. If you are working with a local GPU, the average cost is around $450. Furthermore, you need to make sure that your GPU is compatible with the CUDA toolkit, which are compatible with the libraries that support GPU computing. M1 chips, for example, are not compatible with CUDA toolkit, prohibiting some GPU computing libraries from being used.

## Why GPU is becoming popular - [CUDA](https://developer.nvidia.com/blog/cuda-refresher-the-gpu-computing-ecosystem/)

CUDA is a parallel computing platform and programming model developed by NVIDIA for general computing on graphical processing units (GPUs). With CUDA, developers are able to dramatically speed up computing applications by harnessing the power of GPUs. CUDA gives developers access to the GPUâ€™s virtual instruction set and parallel computational elements, for the execution of compute kernels.

CUDA is a C-like language that is designed to be compiled by a compiler called nvcc. The compiler is able to translate CUDA code into a form that can be executed by the GPU. The compiler also generates the code that is responsible for transferring data between the CPU and GPU.

## What are the tools available for using GPU?

- RAPIDS
- TensorFlow
- PyTorch
- Numba
- CuPy
- PyCuda
- PyMC3 


## References 


